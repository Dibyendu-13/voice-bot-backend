# ðŸŽ™ï¸ Voice Q&A Backend with RAG, Whisper, GPT-4 & TTS

A powerful Node.js backend that turns your voice into answers â€” using OpenAI's Whisper for transcription, Embeddings for context-aware PDF lookup (RAG), GPT-4 for reasoning, and TTS for speaking the answer back.

> ðŸ”— GitHub: [github.com/Dibyendu-13/voice-bot-backend](https://github.com/Dibyendu-13/voice-bot-backend)

---

## âœ¨ Features

- ðŸŽ§ **Voice Input** via microphone (WebM)
- ðŸ§  **Context-Aware Retrieval** from embedded PDFs
- ðŸ¤– **GPT-4 Answers** customized to respond like Dibyendu
- ðŸ”Š **TTS Output** in a realistic male voice (`onyx`)
- ðŸ“¦ **Embeddings Cached** â€” no re-embedding on restart
- âš¡ **Modular Code** (RAG, TTS, Transcription separated)

---

## ðŸ“¦ Tech Stack

- `Node.js`, `Express`, `Multer`
- `OpenAI` (Whisper, GPT-4, TTS, Embeddings)
- `pdf-parse`, `dotenv`, `cors`, `axios`
- `compute-cosine-similarity`

---

## ðŸš€ Getting Started

### 1. Clone the Repository

```bash
git clone https://github.com/Dibyendu-13/voice-bot-backend.git
cd voice-bot-backend
```

### 2. Install Dependencies

```bash
npm install
```

### 3. Set Up Environment Variables

Create a `.env` file in the root:

```env
OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
```

### 4. Add Your PDFs

Place your PDFs (e.g. `sample1.pdf`, `sample2.pdf`) in the project root. Then open `server.js` and edit:

```js
await initRAG(["sample1.pdf", "sample2.pdf"]);
```

### 5. Run the Server

```bash
npm start
```

Server starts at:  
ðŸ“ `http://localhost:5001`

---

## ðŸ§  How Embedding Works

- On first run, PDFs are:
  - Parsed â†’ Chunked â†’ Embedded â†’ Saved to `embeddings.json`
- On future runs:
  - Embeddings are loaded from disk â€” fast startup
- To force re-embedding:
  - Delete `embeddings.json` manually

---

## ðŸŽ§ Voice Flow

1. User speaks (via frontend)
2. Audio is uploaded (`/ask`)
3. Transcribed with Whisper
4. Best-matching chunks are retrieved using cosine similarity
5. GPT-4 generates an answer (in Dibyendu's voice)
6. Answer is spoken back using OpenAI TTS

---

## ðŸ“‚ Project Structure

```
voice-bot-backend/
â”œâ”€â”€ server.js               # Main Express server
â”œâ”€â”€ .env                    # OpenAI API key
â”œâ”€â”€ embeddings.json         # Auto-generated on first run
â”œâ”€â”€ sample1.pdf             # Custom documents
â”œâ”€â”€ sample2.pdf
â”œâ”€â”€ uploads/                # Audio input/output
â””â”€â”€ rag/                    # Modular RAG engine
    â”œâ”€â”€ embedder.js
    â”œâ”€â”€ store.js
    â””â”€â”€ index.js
```

---

## ðŸ”— API Endpoint

**POST /ask**

- Content-Type: `multipart/form-data`
- Form field: `audio` (WebM file)

### Example Response:

```json
{
  "question": "What is the purpose of this document?",
  "answer": "Based on the context, the document outlines...",
  "audio": "uploads/resp-1682823788.mp3"
}
```

---

## ðŸ§‘ Voice Personality

This assistant speaks like **Dibyendu** â€” a highly disciplined, sociable, and detail-oriented Full Stack Developer. Responses are practical, clear, and grounded in real-world logic.

---

## ðŸ“ License

MIT Â© [Dibyendu-13](https://github.com/Dibyendu-13)

---

## ðŸŒŸ Star this repo if you find it helpful!

> GitHub: [https://github.com/Dibyendu-13/voice-bot-backend](https://github.com/Dibyendu-13/voice-bot-backend)
